{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQPdi3mkVJMMhYnGGi6TXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aycaaozturk/Image-Based-Detection-of-Nail-Melanoma-Using-Deep-Learning-Techniques/blob/main/XAI_ViT_b_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the trained Vision Transformer, runs it on a desired image, collects the self attention matrices (attention rollout) and gives a heatmap, showing which regions influenced the prediction"
      ],
      "metadata": {
        "id": "i_c8en554zo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MiruGfUwT54",
        "outputId": "8c2db1dc-3666-4d50-f25c-54a40819bba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)   # to kill the process"
      ],
      "metadata": {
        "id": "2iC9cxTKxUXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "from torch.nn import MultiheadAttention\n",
        "from torchvision import models\n",
        "from torchvision.models.vision_transformer import VisionTransformer, EncoderBlock\n",
        "\n",
        "\n",
        "# 1. Monkey-patch MultiheadAttention -> we change the original function so that we can capture the attention matrice\n",
        "\n",
        "_old_mha_forward = MultiheadAttention.forward  # saves the original forward method of MultiheadAttention as old mha forward\n",
        "\n",
        "def _patched_mha_forward(self, query, key, value, *args, **kwargs): # we define a new forward function\n",
        "    # ensure attention weights always returned\n",
        "\n",
        "    kwargs['need_weights'] = True  # return attention weights with the output\n",
        "\n",
        "    kwargs['average_attn_weights'] = False  # dont average over heads\n",
        "\n",
        "    out, attn = _old_mha_forward(self, query, key, value, *args, **kwargs)  # original forward function\n",
        "    self.last_attn = attn   # stores the last attention weigths into the attribute\n",
        "    return out, attn    # returns output + attention weigths\n",
        "\n",
        "MultiheadAttention.forward = _patched_mha_forward   # -> multiheadattention layers will use this function\n",
        "\n",
        "\n",
        "\n",
        "# 2. Load full ViT model from the path\n",
        "\n",
        "def load_vit_full(model_path, device):\n",
        "    torch.serialization.add_safe_globals([VisionTransformer])\n",
        "    model = torch.load(model_path, weights_only=False, map_location=device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# 3. Preprocessing\n",
        "# we get the same preprocessing as the training\n",
        "\n",
        "def get_preprocess():\n",
        "    weights = models.ViT_B_16_Weights.DEFAULT   # we get the default pretrained configuration\n",
        "    return weights.transforms()\n",
        "\n",
        "\n",
        "\n",
        "# 4. Attention Rollout implementation (Torchvision ViT)\n",
        "\n",
        "#an explainability technique for Transformer based models (e.g. ViT)\n",
        "#that aggregates the attention information across all layers to produce a single, global importance map.\n",
        "\n",
        "def attention_rollout_torchvision(model, x, discard_ratio=0.9, head_fusion=\"mean\"):\n",
        "    device = x.device\n",
        "\n",
        "    # 1) Forward pass so .last_attn is populated\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "\n",
        "    # 2) Collect all attention blocks\n",
        "    attn_blocks = []\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, EncoderBlock):\n",
        "            attn = module.self_attention.last_attn\n",
        "            if attn is not None:\n",
        "                attn_blocks.append(attn[0].to(device))  # [heads, tokens, tokens]\n",
        "\n",
        "    if len(attn_blocks) == 0:\n",
        "        raise RuntimeError(\"No attention matrices collected. Patch may not have applied.\")\n",
        "\n",
        "    # 3) Rollout calculation\n",
        "    tokens = attn_blocks[0].size(-1)\n",
        "    result = torch.eye(tokens, device=device)\n",
        "\n",
        "    for attn in attn_blocks:\n",
        "        if head_fusion == \"mean\":\n",
        "            attn_fused = attn.mean(dim=0)\n",
        "        elif head_fusion == \"max\":\n",
        "            attn_fused = attn.max(dim=0).values\n",
        "        else:\n",
        "            attn_fused = attn.min(dim=0).values\n",
        "\n",
        "        flat = attn_fused.flatten()\n",
        "        k = int(flat.numel() * discard_ratio)\n",
        "\n",
        "        if k > 0:\n",
        "            _, idxs = flat.topk(k, largest=False)\n",
        "            attn_fused = attn_fused.reshape(-1)\n",
        "            attn_fused[idxs] = 0\n",
        "            attn_fused = attn_fused.view(tokens, tokens)\n",
        "\n",
        "        attn_fused = attn_fused / (attn_fused.sum(dim=-1, keepdim=True) + 1e-6)\n",
        "\n",
        "        result = result @ (attn_fused + torch.eye(tokens, device=device))\n",
        "\n",
        "    mask = result[0, 1:]  # remove CLS token\n",
        "\n",
        "    side = int(np.sqrt(mask.numel()))\n",
        "    return mask[:side * side].reshape(side, side).cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# 5. Full pipeline (image → rollout → heatmap)\n",
        "\n",
        "def explain_image(model_path, image_path, save_path=\"rollout.jpg\", discard_ratio=0.8):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model\n",
        "    model = load_vit_full(model_path, device)\n",
        "\n",
        "    preprocess = get_preprocess()\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    x = preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "    print(\"Computing attention rollout...\")\n",
        "    mask = attention_rollout_torchvision(model, x, discard_ratio=discard_ratio)\n",
        "\n",
        "    mask_resized = cv2.resize(mask, img.size)\n",
        "    mask_resized = (mask_resized - mask_resized.min()) / (mask_resized.max() - mask_resized.min())\n",
        "\n",
        "    img_np = np.array(img)\n",
        "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "    heatmap = cv2.applyColorMap((mask_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(img_bgr, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "    cv2.imwrite(save_path, overlay)\n",
        "    print(f\"Saved rollout visualization → {save_path}\")\n",
        "\n",
        "    return overlay\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 6. Example usage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    explain_image(\n",
        "        model_path=\"/content/drive/My Drive/models best/vision transformer/nail_vit_full_model.pth\",\n",
        "        image_path=\"/content/drive/My Drive/models best/vision transformer/pics/cropped_nail2.jpg\",\n",
        "        save_path=\"attention_rollout_vit_b16_nail.jpg\",\n",
        "        discard_ratio=0.8\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc5khFHqwbK6",
        "outputId": "1e3548c2-981f-4339-b90b-f0f3ef38ff22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing attention rollout...\n",
            "Saved rollout visualization → attention_rollout_vit_b16_nail.jpg\n"
          ]
        }
      ]
    }
  ]
}